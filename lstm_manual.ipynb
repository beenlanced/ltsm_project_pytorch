{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4692fa08",
   "metadata": {},
   "source": [
    "# Building, Training, and Executing a Long Short-Term Memory (LSTM) Model Manually \n",
    "\n",
    "This notebook builds a LSTM Model manually by creating classes.\n",
    "\n",
    "In a second notebook, I will use some built in PyTorch, Lightning functions to build the same function, showing how much easier it can be using built in functions. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaff20",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "I had copious comments to help better understand what each import is accomplishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f768be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L # Lightning has tons of cool tools that make neural networks easier\n",
    "\n",
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time I am using Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader # needed for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbd1e6",
   "metadata": {},
   "source": [
    "----\n",
    "## Example - Building a Long Short-Term Memory Unit one Component at a time using `PyTorch and Lightning`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc2adc",
   "metadata": {},
   "source": [
    "### Creating the LSTM Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ff5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # set the seed for the random number generator.\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        ###################\n",
    "        #\n",
    "        # Initialize the tensors for the LSTM\n",
    "        #\n",
    "        ###################\n",
    "\n",
    "        # Using random values to initialize the tensors\n",
    "        # Here are two 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        # Start with the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        # Use the normal distribution for the Weights.\n",
    "        # All Biases are initialized to 0.\n",
    "        #\n",
    "        # These are the Weights and Biases in the first stage, which determines what percentage\n",
    "        # of the long-term memory the LSTM unit will remember.\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # These are the Weights and Biases in the second stage, which determins the new\n",
    "        # potential long-term memory and what percentage will be remembered.\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # These are the Weights and Biases in the third stage, which determines the\n",
    "        # new short-term memory and what percentage will be sent to the output.\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # We can also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        # how nn.LSTM() does it.\n",
    "#         self.wlr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wlr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.blr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wpr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        # lstm_unit does the math for a single LSTM unit.\n",
    "\n",
    "        # NOTES:\n",
    "        # long term memory is also called \"cell state\"\n",
    "        # short term memory is also called \"hidden state\"\n",
    "\n",
    "        # 1) The first stage determines what percent of the current long-term memory\n",
    "        #    should be remembered\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) +\n",
    "                                              (input_value * self.wlr2) +\n",
    "                                              self.blr1)\n",
    "\n",
    "        # 2) The second stage creates a new, potential long-term memory and determines what\n",
    "        #    percentage of that to add to the current long-term memory\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +\n",
    "                                                   (input_value * self.wpr2) +\n",
    "                                                   self.bpr1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) +\n",
    "                                      (input_value * self.wp2) +\n",
    "                                      self.bp1)\n",
    "\n",
    "        # Once we have gone through the first two stages, we can update the long-term memory\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) +\n",
    "                       (potential_remember_percent * potential_memory))\n",
    "\n",
    "        # 3) The third stage creates a new, potential short-term memory and determines what\n",
    "        ##    percentage of that should be remembered and used as output.\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) +\n",
    "                                       (input_value * self.wo2) +\n",
    "                                       self.bo1)\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        # Finally, we return the updated long and short-term memories\n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data\n",
    "        # that we have. forward() also keeps track of the long and short-term memories after each day and returns\n",
    "        # the final short-term memory, which is the 'output' of the LSTM.\n",
    "\n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "\n",
    "        # Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "\n",
    "        # Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "\n",
    "        # Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "\n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        ##### Now return short_memory, which is the 'output' of the LSTM.\n",
    "        return short_memory\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # NOTE: Setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more\n",
    "                                                 # training. However, if we use the default value, we get\n",
    "                                                 # the exact same Weights and Biases that I used in\n",
    "                                                 # the LSTM Clearly Explained StatQuest video. So we'll use the\n",
    "                                                 # default value.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "\n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        ## NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        ## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        ## the goal is to predict that the value on Day 5 = 1. We use label_i, the value we want to\n",
    "        ## predict, to keep track of which company we just made a prediction for and\n",
    "        ## log that output value in a company specific file\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

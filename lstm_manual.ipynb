{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4692fa08",
   "metadata": {},
   "source": [
    "# Building, Training, and Executing a Long Short-Term Memory (LSTM) Model Manually \n",
    "\n",
    "This notebook builds a LSTM Model manually by creating classes.\n",
    "\n",
    "In a second notebook, I will use some built in PyTorch, Lightning functions to build the same model, showing how much easier it can be using built-in functions. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaff20",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "I have added copious comments to help better understand what each import is accomplishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f768be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L # Lightning has tons of cool tools that make neural networks easier\n",
    "\n",
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time I am using Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader # needed for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbd1e6",
   "metadata": {},
   "source": [
    "----\n",
    "## Example - Building a Long Short-Term Memory Unit one Component at a time using `PyTorch and Lightning`\n",
    "\n",
    "\n",
    "For this LSTM example, I imagine that I have two companies: Company A and Company B with five day's worth of stock prices\n",
    "\n",
    "![company stock](imgs/company_stock_prices.png)\n",
    "\n",
    "Given this sequential data, I want to see if I can get the LSTM to remember what happened on Day 1 through Day 4, to see if I can correctly predict what will happen on Day 5. \n",
    "\n",
    "`The objective`: I run the data from Day 1 through Day 4 through the LSTM to see If I can predict the values for Day 5 for both Company A and Company B.\n",
    "\n",
    "For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,the goal is to predict that the value on Day 5 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc2adc",
   "metadata": {},
   "source": [
    "### Creating the LSTM Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ff5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # set the seed for the random number generator.\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        ###################\n",
    "        #\n",
    "        # Initialize the tensors for the LSTM\n",
    "        #\n",
    "        ###################\n",
    "\n",
    "        # Using random values to initialize the tensors\n",
    "        # Here are two 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        # I use the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        # Use the normal distribution for the Weights.\n",
    "        # All Biases are initialized to 0.\n",
    "        #\n",
    "        # These are the Weights and Biases in the first stag that determines what percentage\n",
    "        # of the long-term memory the LSTM unit will remember.\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # These are the Weights and Biases in the second stage that determine the new\n",
    "        # potential long-term memory and what percentage will be remembered.\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # These are the Weights and Biases in the third stage that determine the\n",
    "        # new short-term memory and what percentage will be sent to the output.\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        # I could also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        # how the automated function nn.LSTM() does it. Adding the code here to show what that might look like.\n",
    "#         self.wlr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wlr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.blr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wpr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        # lstm_unit does the math for a single LSTM unit.\n",
    "\n",
    "        # NOTES:\n",
    "        # long term memory is also called \"cell state\"\n",
    "        # short term memory is also called \"hidden state\"\n",
    "\n",
    "        # 1) The first stage determines what percent of the current long-term memory\n",
    "        #    should be remembered\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) +\n",
    "                                              (input_value * self.wlr2) +\n",
    "                                              self.blr1)\n",
    "\n",
    "        # 2) The second stage creates a new, potential long-term memory and determines what\n",
    "        #    percentage of that to add to the current long-term memory\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +\n",
    "                                                   (input_value * self.wpr2) +\n",
    "                                                   self.bpr1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) +\n",
    "                                      (input_value * self.wp2) +\n",
    "                                      self.bp1)\n",
    "\n",
    "        # Once we have gone through the first two stages, we can update the long-term memory\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) +\n",
    "                       (potential_remember_percent * potential_memory))\n",
    "\n",
    "        # 3) The third stage creates a new, potential short-term memory and determines what\n",
    "        ##    percentage of that should be remembered and used as output.\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) +\n",
    "                                       (input_value * self.wo2) +\n",
    "                                       self.bo1)\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        # Finally, we return the updated long and short-term memories\n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "    \n",
    "    def forward(self, input: list[int]) -> list[int]:\n",
    "        # forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data\n",
    "        # that I have. forward() also keeps track of the long and short-term memories after each day and returns\n",
    "        # the final short-term memory, which is the 'output' of the LSTM.\n",
    "\n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., hN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "\n",
    "        # Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "\n",
    "        # Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "\n",
    "        # Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "\n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        # Now return short_memory, which is the 'output' of the LSTM.\n",
    "        return short_memory\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # NOTE: Setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more\n",
    "                                                 # training. For now, just going to use the default\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "\n",
    "        ###################\n",
    "        #\n",
    "        # Logging the loss and the predicted values so we can evaluate the training\n",
    "        #\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        # NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        # For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        # the goal is to predict that the value on Day 5 = 1. We use label_i, the value we want to\n",
    "        # predict, to keep track of which company we just made a prediction for and\n",
    "        # log that output value in a company specific file\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb73bd",
   "metadata": {},
   "source": [
    "## Creating an Instance for the LSTM\n",
    "\n",
    "Now, that I have created class ,`LSTMbyHand`, that defines an LSTM, I can use it to create a model and print out the randomly initialized `Weights` and `Biases`. \n",
    "\n",
    "Then, just for fun, I'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then I am done! However, the chances of getting good predictions from random values is very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2ff58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "wlr1 tensor(0.3367)\n",
      "wlr2 tensor(0.1288)\n",
      "blr1 tensor(0.)\n",
      "wpr1 tensor(0.2345)\n",
      "wpr2 tensor(0.2303)\n",
      "bpr1 tensor(0.)\n",
      "wp1 tensor(-1.1229)\n",
      "wp2 tensor(-0.1863)\n",
      "bp1 tensor(0.)\n",
      "wo1 tensor(2.2082)\n",
      "wo2 tensor(-0.6380)\n",
      "bo1 tensor(0.)\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(-0.0377)\n",
      "Company B: Observed = 1, Predicted = tensor(-0.0383)\n"
     ]
    }
   ],
   "source": [
    "# Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = LSTMbyHand()\n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "# NOTE: To make predictions, we pass in the first 4 days worth of stock values\n",
    "# in an array for each company. In this case, the only difference between the\n",
    "# input values for Company A and B occurs on the first day. Company A has 0 and\n",
    "# Company B has 1.\n",
    "print(\"Company A: Observed = 0, Predicted =\",\n",
    "      model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\",\n",
    "      model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd73ddc",
   "metadata": {},
   "source": [
    "## Initial Results \n",
    "With the unoptimized paramters (i.e., using the initial random weights), the predicted value for **Company A**, **-0.0377**, isn't terrible, since it is relatively close to the observed value, **0**. However, the predicted value for **Company B**, **-0.0383**, _is_ terrible, because it is relatively far from the observed value, **1**. So, that means I need to train the LSTM.\n",
    "\n",
    "Note, we would still want to train, but it was a first attempt to see if our first attempt was close enough or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa49cb4",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Time to Train my LSTM\n",
    "\n",
    "Train the LSTM unit and use `Lightning` and `TensorBoard` to evaluate\n",
    "\n",
    "\n",
    "\n",
    "### Use `DataLoader`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data as a tensor for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]]) #A and B\n",
    "labels = torch.tensor([0., 1.]) # Anticipated output predictions for company A and company B\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95162d",
   "metadata": {},
   "source": [
    "Next, I have to create a `Lightning Trainer`.\n",
    "\n",
    "* `L.Trainer` - A Class that I use to facilitate training of the data\n",
    "    * I start with 2000 epochs, which may or may not be good enough\n",
    "    * Recall, I used the standard learning rate, 0.001, which makes learning slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da87483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/lancehester/Documents/ltsm_project_pytorch/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/Users/lancehester/Documents/ltsm_project_pytorch/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fec890f37047da8bf517708c93afd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=2000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e76e5",
   "metadata": {},
   "source": [
    "### Okay, now I that I have trained the model with 2000 Epochs, I can see how good the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da5b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.4342)\n",
      "Company B: Observed = 1, Predicted = tensor(0.6171)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4863d",
   "metadata": {},
   "source": [
    "### Summarizing these results:\n",
    "\n",
    "* The predictions are terrible. \n",
    "    * Company A - Day 5 prediction is 0.4342 -- very far from 0\n",
    "    * Company B - Day 5 prediction is 0.6171 -- very far from 1\n",
    "\n",
    "* Conclusion:\n",
    "    * I have more training to do. \n",
    "    * A good place to start is to look at the `loss` values and `predictions` that were saved in the log files using `TensorBoard`\n",
    "\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard) is a visualization toolkit for TensorFlow that provides tools and visualizations for machine learning experimentation. It is particularly useful for understanding, debugging, and optimizing machine learning models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76716fbc",
   "metadata": {},
   "source": [
    "### To get TensorBoard working with VS code\n",
    "\n",
    "* Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "    * you may need to add tensorboard to your current virtual environment\n",
    "        * in terminal I used `uv add tensorbard` as I use uv to add modules\n",
    "        * Note: all of this should be done for you with this project as all dependencies are in the pyproject.toml file. \n",
    "\n",
    "   * I had to `restart and run the code` in the notebook and it resulted in TensorBoard VS Code extension addition message \n",
    "\n",
    "   * Or if need be: Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "   \n",
    "   * You will be able to select the folder where your TensorBoard log files are located. By default, the current working directory will be used. Here, I used the `lightning_logs` directory\n",
    "\n",
    "    * VS Code will then open a new tab with TensorBoard and its lifecycle will be managed by VS Code as well. This tab means that to kill the TensorBoard process all you have to do is close the TensorBoard tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff90aa9",
   "metadata": {},
   "source": [
    "### Looking at the TensorBoard Training Loss Results\n",
    "\n",
    "In the figures below, I show the TensorBoard loss( train_loss) figure and\n",
    "* The predictions for Company A(out_0) and the predictions for Company B(out_1).\n",
    "    * note the X - axis refers to the epoch runs\n",
    "    * note the Y - axis refers to the stock prediction values\n",
    "* Recall:\n",
    "    * Company A, I want to predict 0\n",
    "    * Company B, I want to predict 1\n",
    "\n",
    "![train_loss](imgs/tensorboard_train_loss_part1.png)\n",
    "\n",
    "![out_0](imgs/out_0_part1.png)\n",
    "![out_0](imgs/out_1_part1.png)\n",
    "\n",
    "\n",
    "#### Discussion of the Loss\n",
    "The **loss** (`train_loss`) is going down, which is good, but it still has further to go.\n",
    "\n",
    "#### Discussion of the Out_0 and Out_1\n",
    "\n",
    "* When I look at the predictions for **Company A** (`out_0`), I see that the predictions started out pretty good, close to **0**, but then got really bad early on in training, shooting all the way up to **0.5**, but are starting to get smaller.\n",
    "\n",
    "* In contrast, when I look at the predictions for **Company B** (`out_1`), I see that predictions started out really bad, close to **0**, but have been getting better ever since and look like they could continue to get better if I kept training.\n",
    "\n",
    "#### Summarization\n",
    "I need to peform more training to improve predictions. So, time to add more epochs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff7040",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Optimizing (Training) the Weights and Biases in the LSTM: Adding More Epochs without Starting Over\n",
    "\n",
    "\n",
    "The good news is that because I am using **Lightning**, I can pick up where I left off training without having to start over from scratch. \n",
    "\n",
    "This capability is because when I train with **Lightning**, it creates _checkpoint_ files that keep track of the `Weights` and `Biases` as they change. As a result, all I have to do to is pick up where I left off is to tell the `Trainer` where the checkpoint files are located. \n",
    "\n",
    "How awesome, as this capability will save me a lot of time since I don't have to retrain the first **2000** epochs. \n",
    "\n",
    "So, I am going to add an additional **1000** epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752706e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_1/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/Users/lancehester/Documents/ltsm_project_pytorch/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_1/checkpoints' to '/Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_1/checkpoints/epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_1/checkpoints/epoch=1999-step=4000.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352941269c6f4eeda2f87037421a61b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    }
   ],
   "source": [
    "# First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" +\n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=3000) # Before, max_epochs=2000, so, by setting it to 3000, I am adding 1000 more.\n",
    "# And then I call fit() using the path to the most recent checkpoint files\n",
    "# so that I can pick up where I left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.2708)\n",
      "Company B: Observed = 1, Predicted = tensor(0.7534)\n"
     ]
    }
   ],
   "source": [
    "# Now that I have added **1000** epochs to the training, I check the predictions...\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222d9fc",
   "metadata": {},
   "source": [
    "#### Results with Additional 1000 epochs\n",
    "\n",
    "The results are better. Checkout the Blue lines. \n",
    "\n",
    "![train_loss](imgs/loss_part_2.png)\n",
    "\n",
    "The loss is getting smaller\n",
    "\n",
    "\n",
    "And predictions are improving. \n",
    "\n",
    "![out_0](imgs/out_0_part2.png)\n",
    "\n",
    "![out_1](imgs/out_1_part2.png)\n",
    "\n",
    "\n",
    "I can do better. Let me add `2000` more epochs to the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_2/checkpoints/epoch=2999-step=6000.ckpt\n",
      "/Users/lancehester/Documents/ltsm_project_pytorch/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_2/checkpoints' to '/Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_2/checkpoints/epoch=2999-step=6000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /Users/lancehester/Documents/ltsm_project_pytorch/lightning_logs/version_2/checkpoints/epoch=2999-step=6000.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f2fe072ffa4fa0ba9ca14e5fa4a732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" +\n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=5000) # Before, max_epochs=3000, so, by setting it to 5000, I am adding 2000 more.\n",
    "## And then I call fit() using the path to the most recent checkpoint files\n",
    "## so that I can pick up where I left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede23aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now, I compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.0022)\n",
      "Company B: Observed = 1, Predicted = tensor(0.9693)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow, I compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3fa11",
   "metadata": {},
   "source": [
    "## The predictions are the best yet (see red lines in the figures).\n",
    "\n",
    "* The prediction for Company A, `0.0022`, is close to 0\n",
    "* The prediction for Company B, `0.9693`, is close to 1\n",
    "\n",
    "The TensorBoard results:\n",
    "\n",
    "![train_loss](imgs/loss_part_3.png)\n",
    "\n",
    "The loss is getting smaller to the point that adding more EPOCHS probably is not going to provde any additional benefits, so I am going to say that we are done with the training phase.  Yahoo!\n",
    "\n",
    "\n",
    "Predictions are good as well. \n",
    "\n",
    "![out_0](imgs/out_0_part3.png)\n",
    "\n",
    "![out_1](imgs/out_1_part3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11745265",
   "metadata": {},
   "source": [
    "Having a look at the final weights and biases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef25b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "wlr1 tensor(2.7043)\n",
      "wlr2 tensor(1.6307)\n",
      "blr1 tensor(1.6234)\n",
      "wpr1 tensor(1.9983)\n",
      "wpr2 tensor(1.6525)\n",
      "bpr1 tensor(0.6204)\n",
      "wp1 tensor(1.4122)\n",
      "wp2 tensor(0.9393)\n",
      "bp1 tensor(-0.3217)\n",
      "wo1 tensor(4.3848)\n",
      "wo2 tensor(-0.1943)\n",
      "bo1 tensor(0.5935)\n"
     ]
    }
   ],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

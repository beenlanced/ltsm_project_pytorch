{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4692fa08",
   "metadata": {},
   "source": [
    "# Building, Training, and Executing a Long Short-Term Memory (LSTM) Model Using Automation Lightning functions \n",
    "\n",
    "This notebook builds a LSTM Model using Lightning Helper functions\n",
    "\n",
    "I am showing how much easier it can be using built in functions. \n",
    "\n",
    "#### Specifically, I am using `PyTorch LSTM, nn.LSTM()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaff20",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "I have added copious comments to help better understand what each import is accomplishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f768be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L # Lightning has tons of cool tools that make neural networks easier\n",
    "\n",
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time I am using Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader # needed for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbd1e6",
   "metadata": {},
   "source": [
    "----\n",
    "## Example - Building a Long Short-Term Memory Unit using `PyTorch and Lightning`\n",
    "\n",
    "\n",
    "For this LSTM example, I imagine that I have two companies: Company A and Company B with five day's worth of stock prices\n",
    "\n",
    "![company stock](imgs/company_stock_prices.png)\n",
    "\n",
    "Given this sequential data, I want to see if I can get the LSTM to remember what happened on Day 1 through Day 4, to see if I can correctly predict what will happen on Day 5. \n",
    "\n",
    "`The objective`: I will run the data from Day 1 through Day 4 through the LSTM to see If I can predict the values for Day 5 for both Company A and Company B.\n",
    "\n",
    "For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,the goal is to predict that the value on Day 5 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc2adc",
   "metadata": {},
   "source": [
    "### Creating the LSTM Model Class\n",
    "# Using and optimzing the PyTorch LSTM, nn.LSTM()\n",
    "\n",
    "Taking advantage of PyTorch's `nn.LSTM()` function. \n",
    "\n",
    "For the most part, using `nn.LSTM()` allows me to simplify the `__init__()` function and the `forward()` function. \n",
    "\n",
    "The other big difference is that this time, I can set the learning rate for the optimizer, Adam, to **0.1**. This change will speed up training a lot. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of coding an LSTM manually, let's see what I can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and I use it to initialize the Weights and Biases.\n",
    "\n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        # input_size = number of features (or variables) in the data. In my example,\n",
    "        #              I only have a single feature (value)\n",
    "        # hidden_size = this determines the dimension of the output\n",
    "        #               in other words, if I set hidden_size=1, then I will have 1 output node\n",
    "        #               if I set hiddeen_size=50, then I will have 50 output nodes (that can then be 50 input\n",
    "        #               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "\n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "\n",
    "        # lstm_out has the short-term memories for all inputs. I make my prediction with the last one\n",
    "        prediction = lstm_out[-1]\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): # This method configures the optimizer I want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) # Set the learning rate to 0.1\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # Take a step during gradient descent.\n",
    "        input_i, label_i = batch # Collect input\n",
    "        output_i = self.forward(input_i[0]) # Run input through the neural network\n",
    "        loss = (output_i - label_i)**2 # Loss = squared residual\n",
    "\n",
    "        ###################\n",
    "        #\n",
    "        # Logging the loss and the predicted values so I can evaluate the training\n",
    "        #\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb73bd",
   "metadata": {},
   "source": [
    "## Creating an Instance for the LSTM\n",
    "\n",
    "Now, that I have created class ,`LightningLSTM`, that defines an LSTM, I can use it to create a model and print out the randomly initialized `Weights` and `Biases`. \n",
    "\n",
    "Then, just for fun, I'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then I am done! However, the chances of getting good predictions from random values is very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2ff58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
      "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
     ]
    }
   ],
   "source": [
    "# Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = LightningLSTM()\n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "# NOTE: To make predictions, we pass in the first 4 days worth of stock values\n",
    "# in an array for each company. In this case, the only difference between the\n",
    "# input values for Company A and B occurs on the first day. Company A has 0 and\n",
    "# Company B has 1.\n",
    "print(\"Company A: Observed = 0, Predicted =\",\n",
    "      model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\",\n",
    "      model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd73ddc",
   "metadata": {},
   "source": [
    "## Initial Results \n",
    "With the unoptimized paramters (i.e., using the initial random weights), the predicted value for **Company A**, **-0.6675**, isn't terrible, since it is relatively close to the observed value, **0**. However, the predicted value for **Company B**, **-0.6665**, _is_ bad, because it is relatively far from the observed value, **1**. So, that means I need to train the LSTM.\n",
    "\n",
    "Note, we would still want to train, but it was a first attempt to see if our first attempt was close enough or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa49cb4",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Time to Train my LSTM\n",
    "\n",
    "Train the LSTM unit and use `Lightning` and `TensorBoard` to evaluate\n",
    "\n",
    "\n",
    "\n",
    "### Use `DataLoader`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data as a tensor for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]]) #A and B\n",
    "labels = torch.tensor([0., 1.]) # Anticipated output predictions for company A and company B\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95162d",
   "metadata": {},
   "source": [
    "Next, I have to create a `Lightning Trainer`.\n",
    "\n",
    "* `L.Trainer` - A Class that I use to facilitate training of the data\n",
    "    * I start with 300 epochs, which may or may not be good enough\n",
    "    * Recall, I used the standard learning rate, 0.1, which makes learning fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da87483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111244614e8145bda923c012008b72fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=300)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e76e5",
   "metadata": {},
   "source": [
    "### Okay, now I that I have trained the model with 300 Epochs, I can see how good the predictions are.\n",
    "\n",
    " NOTE: Because I have set Adam's learning rate to 0.1, It will train much, much faster.\n",
    " Before, with the manual made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    " the model. Now, with the learning rate set to 0.1, I only need 300 epochs. \n",
    " \n",
    " Now, because I am doing so few epochs, I have to tell the trainer to add stuff to the log files every 2 steps (or epoch, since I have two rows of training data) because the with default logging settings, updating the log files every 50 steps, results in a terrible looking graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da5b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[3.5364],\n",
      "        [1.3869],\n",
      "        [1.5390],\n",
      "        [1.2488]])\n",
      "lstm.weight_hh_l0 tensor([[5.2070],\n",
      "        [2.9577],\n",
      "        [3.2652],\n",
      "        [2.0678]])\n",
      "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
      "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n"
     ]
    }
   ],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([6.7842e-05])\n",
      "Company B: Observed = 1, Predicted = tensor([0.9809])\n"
     ]
    }
   ],
   "source": [
    "# Now that training is done, I print out the new predictions...\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4863d",
   "metadata": {},
   "source": [
    "### Summarizing these results after 300 epochs:\n",
    "\n",
    "* The predictions are great. \n",
    "    * Company A - Day 5 prediction is 6.7842e-05 --  much closer to 0\n",
    "    * Company B - Day 5 prediction is 0.9809 -- very close to 1\n",
    "\n",
    "* TensorBoard\n",
    "    * Have a look at the `loss` values and `predictions` that were saved in the log files using `TensorBoard`\n",
    "\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard) is a visualization toolkit for TensorFlow that provides tools and visualizations for machine learning experimentation. It is particularly useful for understanding, debugging, and optimizing machine learning models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76716fbc",
   "metadata": {},
   "source": [
    "### To get TensorBoard working with VS code\n",
    "\n",
    "* Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "    * you may need to add tensorboard to your current virtual environment\n",
    "        * in terminal I used `uv add tensorbard` as I use uv to add modules\n",
    "        * Note: all of this should be done for you with this project as all dependencies are in the pyproject.toml file. \n",
    "\n",
    "   * I had to `restart and run the code` in the notebook and it resulted in TensorBoard VS Code extension addition message \n",
    "\n",
    "   * Or if need be: Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "   \n",
    "   * You will be able to select the folder where your TensorBoard log files are located. By default, the current working directory will be used. Here, I used the `lightning_logs` directory\n",
    "\n",
    "    * VSCode will then open a new tab with TensorBoard and its lifecycle will be managed by VS Code as well. This tab means that to kill the TensorBoard process all you have to do is close the TensorBoard tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff90aa9",
   "metadata": {},
   "source": [
    "### Looking at the TensorBoard Training Loss Results\n",
    "\n",
    "In the figures below, I show the TensorBoard loss (train_loss) figure and\n",
    "* The predictions for Company A(out_0) and the predictions for Company B(out_1).\n",
    "    * note the X - axis refers to the epoch runs\n",
    "    * note the Y - axis refers to the stock prediction values\n",
    "* Recall:\n",
    "    * Company A, I want to predict 0\n",
    "    * Company B, I want to predict 1\n",
    "\n",
    "![train_loss](imgs/loss_part_4.png)\n",
    "\n",
    "![out_0](imgs/out_0_part4.png)\n",
    "\n",
    "![out_1](imgs/out_1_part4.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Summary\n",
    "In all three figures, the loss (`train_loss`) and the predictions for Company A (`out_0`) and Company B (`out_1`) started to taper off after 500 steps or 250 epochs (recall, I used every two steps), suggesting that adding more epochs may not improve the predictions much, so I am done. I have built the 5th day LSTM predictor!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
